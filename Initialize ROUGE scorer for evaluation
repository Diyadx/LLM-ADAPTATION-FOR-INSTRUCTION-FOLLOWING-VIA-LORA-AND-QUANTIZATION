# Initialize ROUGE scorer for evaluation - Complete runnable example
!pip install -q rouge-score pandas

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from rouge_score import rouge_scorer
import numpy as np
import pandas as pd
import time

# Load a model - using a small pre-trained model for example
print("Loading model...")
model_id = "EleutherAI/pythia-70m"  # Small model that should load quickly
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_id)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
model.eval()
print("Model loaded successfully")

# Generate response function
def generate_response(instruction, max_length=50):
    """Generate a response for the given instruction"""
    formatted_input = f"### Instruction:\n{instruction}\n\n### Response:"
    inputs = tokenizer(formatted_input, return_tensors="pt").to(model.device)

    start_time = time.time()
    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=max_length,
            temperature=0.7,
            top_p=0.9,
            do_sample=True
        )
    generation_time = time.time() - start_time

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    if "### Response:" in response:
        response = response.split("### Response:", 1)[1].strip()
    else:
        response = response.replace(formatted_input, "").strip()

    return response, generation_time

# ROUGE evaluation function
def evaluate_with_rouge(model_responses, reference_texts):
    """Calculate ROUGE scores between model responses and references"""
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

    all_scores = []
    for response, reference in zip(model_responses, reference_texts):
        scores = scorer.score(reference, response)
        all_scores.append({
            'response': response,
            'reference': reference,
            'rouge1_f': scores['rouge1'].fmeasure,
            'rouge2_f': scores['rouge2'].fmeasure,
            'rougeL_f': scores['rougeL'].fmeasure
        })

    results_df = pd.DataFrame(all_scores)

    # Calculate aggregate scores
    aggregate_scores = {
        'rouge1_f': np.mean(results_df['rouge1_f']),
        'rouge2_f': np.mean(results_df['rouge2_f']),
        'rougeL_f': np.mean(results_df['rougeL_f'])
    }

    return aggregate_scores, results_df

# Create a small test dataset
test_data = [
    {
        "instruction": "What is machine learning?",
        "response": "Machine learning is a branch of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed."
    },
    {
        "instruction": "Explain the concept of a neural network.",
        "response": "A neural network is a computational model inspired by the human brain, consisting of interconnected nodes or 'neurons' that process and transmit information."
    },
    {
        "instruction": "How does fine-tuning work?",
        "response": "Fine-tuning is the process of taking a pre-trained model and further training it on a specific dataset to adapt it for a particular task or domain."
    },
    {
        "instruction": "What is transfer learning?",
        "response": "Transfer learning is a machine learning technique where a model developed for one task is reused as the starting point for a model on a second task."
    }
]

# Run the evaluation
print("Starting evaluation...")
model_responses = []
reference_texts = []

# Generate responses for each test example
for example in test_data:
    instruction = example["instruction"]
    reference = example["response"]

    print(f"\nInstruction: {instruction}")
    response, gen_time = generate_response(instruction)

    model_responses.append(response)
    reference_texts.append(reference)

    print(f"Reference: {reference}")
    print(f"Response: {response}")
    print(f"Generation time: {gen_time:.2f} seconds")
    print("-" * 50)

# Calculate ROUGE scores
print("\nCalculating ROUGE scores...")
aggregate_scores, results_df = evaluate_with_rouge(model_responses, reference_texts)

# Print results
print("\nROUGE Scores:")
for metric, score in aggregate_scores.items():
    print(f"{metric}: {score:.4f}")

print("\nDetailed results:")
print(results_df[['rouge1_f', 'rouge2_f', 'rougeL_f']])

print("\nEvaluation complete!")
