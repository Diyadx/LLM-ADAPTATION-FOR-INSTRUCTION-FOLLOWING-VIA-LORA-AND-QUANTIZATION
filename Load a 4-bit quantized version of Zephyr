# Load a 4-bit quantized version of Zephyr (based on Mistral architecture)
model_id = "HuggingFaceH4/zephyr-7b-beta"  # Open model based on Mistral

# Configure 4-bit quantization
bnb_config = transformers.BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"  # Padding on the right for generation tasks

# Attempt to load model with quantization if CUDA is available
try:
    if torch.cuda.is_available():
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True
        )
        print("Successfully loaded 4-bit quantized model")
    else:
        print("CUDA not available. Loading smaller model instead...")
        model_id = "EleutherAI/pythia-410m"  # Fallback to smaller model
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            device_map="auto",
            trust_remote_code=True
        )

        # Update tokenizer to match the new model
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.padding_side = "right"
except Exception as e:
    print(f"Error loading model: {e}")
    print("Falling back to smaller model...")
    model_id = "EleutherAI/pythia-410m"  # Fallback to smaller model
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        device_map="auto",
        trust_remote_code=True
    )

    # Update tokenizer to match the new model
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

# Print model parameters
print(f"Model parameters: {model.num_parameters():,}")
