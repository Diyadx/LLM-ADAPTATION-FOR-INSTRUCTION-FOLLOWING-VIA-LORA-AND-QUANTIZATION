# Minimal training arguments that should work across versions
training_args = transformers.TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=8,
    logging_dir="./logs",
    logging_steps=100,
    save_steps=200,
    save_total_limit=3,
    learning_rate=2e-4,
    weight_decay=0.01,
    warmup_steps=100,
    fp16=True
)

# Create Trainer
trainer = transformers.Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_eval
)
