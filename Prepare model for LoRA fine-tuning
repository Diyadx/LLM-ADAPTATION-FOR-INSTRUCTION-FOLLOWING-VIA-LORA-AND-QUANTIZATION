# Prepare model for LoRA fine-tuning
model = prepare_model_for_kbit_training(model)

# Define LoRA Configuration based on model architecture
# For Mistral, we target the attention modules
if "mistral" in model_id.lower():
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]
else:
    # For other models like Pythia
    target_modules = ["query_key_value"]

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=target_modules,
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

# Apply LoRA to model
model = get_peft_model(model, lora_config)
print(f"Trainable parameters: {model.print_trainable_parameters()}")
